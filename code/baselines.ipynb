{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be7c211c",
   "metadata": {},
   "source": [
    "File to compute the baselines with skitlearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b2e001",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5da0fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1340207:MainThread](2025-05-15 18:34:29,583) INFO - qlib.Initialization - [config.py:420] - default_conf: client.\n",
      "[1340207:MainThread](2025-05-15 18:34:29,583) WARNING - qlib.Initialization - [__init__.py:64] - auto_path is False, please make sure None is mounted\n",
      "[1340207:MainThread](2025-05-15 18:34:29,584) INFO - qlib.Initialization - [__init__.py:74] - qlib successfully initialized based on client settings.\n",
      "[1340207:MainThread](2025-05-15 18:34:29,584) INFO - qlib.Initialization - [__init__.py:76] - data_path={'__DEFAULT_FREQ': PosixPath('/home/gabrielecarrino/.qlib/qlib_data/cn_data')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:  torch.Size([336, 3764, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1340207:MainThread](2025-05-15 18:34:50,134) INFO - qlib.timer - [log.py:127] - Time cost: 0.003s | Loading data Done\n",
      "[1340207:MainThread](2025-05-15 18:34:52,698) INFO - qlib.timer - [log.py:127] - Time cost: 0.088s | Fillna Done\n"
     ]
    }
   ],
   "source": [
    "from master_bert import MASTERModel\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from utils import load_all_csv_data_with_market_indexes, load_all_csv_data_without_index, csvs_to_qlib_df, PandasDataLoader\n",
    "# Please install qlib first before load the data.\n",
    "\n",
    "# Qlib\n",
    "# import qlib\n",
    "# from qlib.config import REG_US           # S&P 500 is a US market\n",
    "# qlib.init(provider_uri=\".\", region=REG_US)   # provider_uri just needs to exist\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1.  Init Qlib and build *one* handler\n",
    "import qlib, pandas as pd, numpy as np, torch\n",
    "qlib.init()                               # client mode is fine\n",
    "\n",
    "from qlib.data.dataset.loader import StaticDataLoader\n",
    "from qlib.data.dataset.handler import DataHandlerLP\n",
    "from qlib.data.dataset import TSDatasetH          # <-- here\n",
    "from qlib.data.dataset.processor import (\n",
    "    DropnaProcessor, CSZScoreNorm, DropnaLabel,\n",
    ")\n",
    "\n",
    "# your tensor, names, dates exactly as before  ----------------\n",
    "stock_tensor, stock_names, feature_names = load_all_csv_data_without_index()\n",
    "# stock_tensor, stock_names, feature_names = load_all_csv_data_with_market_indexes()\n",
    "N, T, K   = stock_tensor.shape\n",
    "print(\"Shape: \", stock_tensor.shape)\n",
    "# dates     = pd.read_csv(\"data/enriched/market_indexes_aggregated.csv\")[\"Date\"]\n",
    "dates = pd.to_datetime(                     # <-- NEW\n",
    "    pd.read_csv(\"data/enriched/market_indexes_aggregated.csv\")[\"Date\"]\n",
    ")\n",
    "\n",
    "# tensor ➜ tidy multi-index frame --------------------------------\n",
    "def tensor_to_df(tensor, inst, feats, dt_index):\n",
    "    flat = tensor.numpy().reshape(N * T, K)\n",
    "    idx  = pd.MultiIndex.from_product([dt_index, inst],\n",
    "                                      names=[\"datetime\", \"instrument\"])\n",
    "    cols = pd.MultiIndex.from_product([[\"feature\"], feats])\n",
    "    return pd.DataFrame(flat, index=idx, columns=cols)\n",
    "\n",
    "df_raw = tensor_to_df(stock_tensor, stock_names, feature_names, dates)\n",
    "\n",
    "# optional: build a forward-return label\n",
    "df_raw[(\"label\", \"FWD_RET\")] = (\n",
    "    df_raw[(\"feature\", \"Adjusted Close\")]\n",
    "      .groupby(\"instrument\").shift(-1) / df_raw[(\"feature\", \"Adjusted Close\")] - 1\n",
    ")\n",
    "\n",
    "# handler with learn / infer processors ------------------------\n",
    "# proc_feat = [\n",
    "#     {\"class\": \"DropnaProcessor\", \"kwargs\": {\"fields_group\": \"feature\"}},\n",
    "#     {\"class\": \"CSZScoreNorm\",   \"kwargs\": {\"fields_group\": \"feature\"}},\n",
    "# ]\n",
    "\n",
    "# proc_feat = [\n",
    "#     {\"class\": \"CSZScoreNorm\",   \"kwargs\": {\"fields_group\": \"feature\"}},\n",
    "# ]\n",
    "\n",
    "proc_feat = [\n",
    "    {\"class\": \"Fillna\",          # <— correct name\n",
    "     \"kwargs\": {\"fields_group\": \"feature\", \"fill_value\": 0}},  # zero-fill; choose ffill/bfill/etc. if you like\n",
    "    {\"class\": \"CSZScoreNorm\",\n",
    "     \"kwargs\": {\"fields_group\": \"feature\"}},\n",
    "]\n",
    "\n",
    "proc_label = [{\"class\": \"DropnaLabel\"}]\n",
    "\n",
    "handler = DataHandlerLP(\n",
    "    data_loader      = StaticDataLoader(df_raw),\n",
    "    infer_processors = proc_feat,          # what the model will see later\n",
    "    learn_processors = proc_feat + proc_label,\n",
    ")\n",
    "handler.fit_process_data()                 # learn z-scores, etc.\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2.  Attach time splits in a TSDatasetH\n",
    "split = {\n",
    "    \"train\": (dates.iloc[0],              dates.iloc[int(T*0.8) - 1]),\n",
    "    \"valid\": (dates.iloc[int(T*0.8)],     dates.iloc[int(T*0.9) - 1]),\n",
    "    \"test\" : (dates.iloc[int(T*0.9)],     dates.iloc[-2]),\n",
    "}\n",
    "\n",
    "ts_ds = TSDatasetH(\n",
    "    handler  = handler,\n",
    "    segments = split,\n",
    "    step_len = 8,          # same window the MASTER code expects\n",
    ")\n",
    "\n",
    "dl_train = ts_ds.prepare(\"train\")   # ➜ TSDataSampler\n",
    "dl_valid = ts_ds.prepare(\"valid\")\n",
    "dl_test  = ts_ds.prepare(\"test\")\n",
    "\n",
    "print(len(dl_train), len(dl_valid), len(dl_test))\n",
    "#  → continue with your for-loop over seeds exactly as before\n",
    "# ------------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be85eaf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample shape: (8, 277)\n",
      "Number of features: 277\n"
     ]
    }
   ],
   "source": [
    "# grab the very first sample\n",
    "sample = dl_train[0]\n",
    "\n",
    "# this will print something like (step_len, num_features)\n",
    "print(\"Sample shape:\", sample.shape)\n",
    "\n",
    "# so the number of features is the second entry:\n",
    "print(\"Number of features:\", sample.shape[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gabri_master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
